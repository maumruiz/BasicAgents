{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aima Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thing(object):\n",
    "\n",
    "    \"\"\"This represents any physical object that can appear in an Environment.\n",
    "    You subclass Thing to get the things you want.  Each thing can have a\n",
    "    .__name__  slot (used for output only).\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<{}>'.format(getattr(self, '__name__', self.__class__.__name__))\n",
    "\n",
    "    def is_alive(self):\n",
    "        \"Things that are 'alive' should return true.\"\n",
    "        return hasattr(self, 'alive') and self.alive\n",
    "\n",
    "    def show_state(self):\n",
    "        \"Display the agent's internal state.  Subclasses should override.\"\n",
    "        print(\"I don't know how to show_state.\")\n",
    "\n",
    "    def display(self, canvas, x, y, width, height):\n",
    "        # Do we need this?\n",
    "        \"Display an image of this Thing on the canvas.\"\n",
    "        pass\n",
    "\n",
    "class Agent(Thing):\n",
    "\n",
    "    \"\"\"An Agent is a subclass of Thing with one required slot,\n",
    "    .program, which should hold a function that takes one argument, the\n",
    "    percept, and returns an action. (What counts as a percept or action\n",
    "    will depend on the specific environment in which the agent exists.)\n",
    "    Note that 'program' is a slot, not a method.  If it were a method,\n",
    "    then the program could 'cheat' and look at aspects of the agent.\n",
    "    It's not supposed to do that: the program can only look at the\n",
    "    percepts.  An agent program that needs a model of the world (and of\n",
    "    the agent itself) will have to build and maintain its own model.\n",
    "    There is an optional slot, .performance, which is a number giving\n",
    "    the performance measure of the agent in its environment.\"\"\"\n",
    "\n",
    "    def __init__(self, program=None):\n",
    "        self.alive = True\n",
    "        self.bump = False\n",
    "        self.holding = []\n",
    "        self.performance = 0\n",
    "        if program is None:\n",
    "            def program(percept):\n",
    "                return eval(input('Percept={}; action? ' .format(percept)))\n",
    "        assert isinstance(program, collections.Callable)\n",
    "        self.program = program\n",
    "\n",
    "    def can_grab(self, thing):\n",
    "        \"\"\"Returns True if this agent can grab this thing.\n",
    "        Override for appropriate subclasses of Agent and Thing.\"\"\"\n",
    "        return False\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "\n",
    "    \"\"\"Abstract class representing an Environment.  'Real' Environment classes\n",
    "    inherit from this. Your Environment will typically need to implement:\n",
    "        percept:           Define the percept that an agent sees.\n",
    "        execute_action:    Define the effects of executing an action.\n",
    "                           Also update the agent.performance slot.\n",
    "    The environment keeps a list of .things and .agents (which is a subset\n",
    "    of .things). Each agent has a .performance slot, initialized to 0.\n",
    "    Each thing has a .location slot, even though some environments may not\n",
    "    need this.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.things = []\n",
    "        self.agents = []\n",
    "\n",
    "    def thing_classes(self):\n",
    "        return []  # List of classes that can go into environment\n",
    "\n",
    "    def percept(self, agent):\n",
    "        '''\n",
    "            Return the percept that the agent sees at this point.\n",
    "            (Implement this.)\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"Change the world to reflect this action. (Implement this.)\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        \"Default location to place a new thing with unspecified location.\"\n",
    "        return None\n",
    "\n",
    "    def exogenous_change(self):\n",
    "        \"If there is spontaneous change in the world, override this.\"\n",
    "        pass\n",
    "\n",
    "    def is_done(self):\n",
    "        \"By default, we're done when we can't find a live agent.\"\n",
    "        return not any(agent.is_alive() for agent in self.agents)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Run the environment for one time step. If the\n",
    "        actions and exogenous changes are independent, this method will\n",
    "        do.  If there are interactions between them, you'll need to\n",
    "        override this method.\"\"\"\n",
    "        if not self.is_done():\n",
    "            actions = []\n",
    "            for agent in self.agents:\n",
    "                if agent.alive:\n",
    "                    actions.append(agent.program(self.percept(agent)))\n",
    "                else:\n",
    "                    actions.append(\"\")\n",
    "            for (agent, action) in zip(self.agents, actions):\n",
    "                self.execute_action(agent, action)\n",
    "            self.exogenous_change()\n",
    "\n",
    "    def run(self, steps=1000):\n",
    "        \"Run the Environment for given number of time steps.\"\n",
    "        for step in range(steps):\n",
    "            if self.is_done():\n",
    "                return\n",
    "            self.step()\n",
    "\n",
    "    def list_things_at(self, location, tclass=Thing):\n",
    "        \"Return all things exactly at a given location.\"\n",
    "        return [thing for thing in self.things\n",
    "                if thing.location == location and isinstance(thing, tclass)]\n",
    "\n",
    "    def some_things_at(self, location, tclass=Thing):\n",
    "        \"\"\"Return true if at least one of the things at location\n",
    "        is an instance of class tclass (or a subclass).\"\"\"\n",
    "        return self.list_things_at(location, tclass) != []\n",
    "\n",
    "    def add_thing(self, thing, location=None):\n",
    "        \"\"\"Add a thing to the environment, setting its location. For\n",
    "        convenience, if thing is an agent program we make a new agent\n",
    "        for it. (Shouldn't need to override this.\"\"\"\n",
    "        if not isinstance(thing, Thing):\n",
    "            thing = Agent(thing)\n",
    "        assert thing not in self.things, \"Don't add the same thing twice\"\n",
    "        thing.location = location if location is not None else self.default_location(thing)\n",
    "        self.things.append(thing)\n",
    "        if isinstance(thing, Agent):\n",
    "            thing.performance = 100\n",
    "            self.agents.append(thing)\n",
    "\n",
    "    def delete_thing(self, thing):\n",
    "        \"\"\"Remove a thing from the environment.\"\"\"\n",
    "        try:\n",
    "            self.things.remove(thing)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(\"  in Environment delete_thing\")\n",
    "            print(\"  Thing to be removed: {} at {}\" .format(thing, thing.location))\n",
    "            print(\"  from list: {}\" .format([(thing, thing.location) for thing in self.things]))\n",
    "        if thing in self.agents:\n",
    "            self.agents.remove(thing)\n",
    "\n",
    "def distance_m(a, b):\n",
    "    \"\"\"The distance between two (x, y) points.\"\"\"\n",
    "    return sum([abs(a[0] - b[0]), abs(a[1] - b[1])])\n",
    "\n",
    "\n",
    "class Trap(Thing):\n",
    "    '''Creates a Gold as a subclass of Thing'''\n",
    "    pass\n",
    "\n",
    "class Gold(Thing):\n",
    "    '''Creates a Trap as a subclass of Thing'''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflex Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexAgent(Agent):\n",
    "    '''\n",
    "    Initializes the Agent's variables. Every Agent starts being alive with a performance of 100 and facing the right direction.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.location = (0,0)\n",
    "        self.direction = 'R'\n",
    "        self.performance = 100\n",
    "        self.alive = True\n",
    "\n",
    "        '''Declares the internal state to register the visited cells into the performance, which will only work to\n",
    "            penalize if it moves to a previously visited cell'''\n",
    "        self.visited_cells = np.full((5, 5), False)\n",
    "        self.visited_cells[self.location[0]][self.location[1]] = True\n",
    "\n",
    "    def __str__(self):\n",
    "        '''Prints the internal state of the agent'''\n",
    "        facing = {'U': 'UP', 'R': 'RIGHT', 'D': 'DOWN', 'L': 'LEFT'}\n",
    "        return \"(%s, %s, %s)\" % (self.location[0], self.location[1], facing[self.direction])\n",
    "\n",
    "    def print_percepts(self, percepts, radius):\n",
    "        '''\n",
    "        Receives a list of percepts around the player and show them.\n",
    "        '''\n",
    "        print(\"Percept\")\n",
    "        \n",
    "        x, y = self.location\n",
    "\n",
    "        '''Gets all the gold perceived around the player'''\n",
    "        gold_grid = [[0 for row in range(5)] for col in range(5)]\n",
    "        '''Gets all the traps perceived around the player'''\n",
    "        traps_grid = [[0 for row in range(5)] for col in range(5)]\n",
    "\n",
    "        for percept in percepts:\n",
    "            xPercept, yPercept = percept[2]\n",
    "            if isinstance(percept[0], Trap):\n",
    "                traps_grid[xPercept][yPercept] += 1\n",
    "            if isinstance(percept[0], Gold):\n",
    "                gold_grid[xPercept][yPercept] += 1\n",
    "\n",
    "        row = '  '\n",
    "        for c in range(-radius, radius+1):\n",
    "            if self.is_inbounds((0, y+c)):\n",
    "                row += '   %s    ' % (y+c)                    \n",
    "        print(row)\n",
    "\n",
    "        for r in range(-radius, radius+1):\n",
    "            if self.is_inbounds((x+r, 0)):\n",
    "                row = '%s ' % (x+r)\n",
    "            \n",
    "                for c in range(-radius, radius+1):\n",
    "                    if self.is_inbounds((x+r, y+c)):\n",
    "                        agentState = '-' if x != x+r or y != y+c else self.direction\n",
    "\n",
    "                        cellGold = gold_grid[x+r][y+c]\n",
    "                        gold = '-' if cellGold < 1 else cellGold\n",
    "\n",
    "                        cellTraps = traps_grid[x+r][y+c]\n",
    "                        traps = '-' if cellTraps < 1 else cellTraps\n",
    "\n",
    "                        row += '(%s %s %s) ' % (agentState, gold, traps)\n",
    "                    \n",
    "                print(row)\n",
    "        print('')\n",
    "\n",
    "    def is_inbounds(self, location):\n",
    "        '''Checks to make sure that the location is inbounds'''\n",
    "        x,y = location\n",
    "        return not (x < 0 or x >= 5 or y < 0 or y >= 5)\n",
    "    \n",
    "    def get_gold(self):\n",
    "        '''States when the agent enters a cell with a gold'''\n",
    "        self.performance += 10\n",
    "\n",
    "    def fall_in_trap(self):\n",
    "        '''Takes 5 points from the agent when the agent enter a cell with traps'''\n",
    "        self.performance -= 5\n",
    "\n",
    "    def turn_clockwise(self):\n",
    "        '''States when the agent turns. \n",
    "        Reduces 1 point from the agent'''\n",
    "        self.performance -= 1\n",
    "        \n",
    "        '''Turns its direction clockwise'''\n",
    "        turns = {'U': 'R', 'R': 'D', 'D': 'L', 'L': 'U'}\n",
    "        self.direction = turns[self.direction]\n",
    "\n",
    "    def move_forward(self):\n",
    "        '''Reduces 1 point from the agent'''\n",
    "        self.performance -= 1\n",
    "\n",
    "        '''Advances in the direction of the agent'''\n",
    "        '''And checks that the agents does not move outside the grid'''\n",
    "        r = self.location[0]\n",
    "        c = self.location[1]\n",
    "\n",
    "        if self.direction == \"U\" and r != 0:\n",
    "            r = r-1\n",
    "        elif self.direction == \"R\" and c != 4:\n",
    "            c = c+1\n",
    "        elif self.direction == \"D\" and r != 4:\n",
    "            r = r+1\n",
    "        elif self.direction == \"L\" and c != 0:\n",
    "            c = c-1\n",
    "        self.location = (r, c)\n",
    "\n",
    "        '''Reduces the performance if the agent enters a visited cell'''\n",
    "        if self.visited_cells[r][c]:\n",
    "            self.performance -= 2\n",
    "        else:\n",
    "            self.visited_cells[r][c] = True\n",
    "    \n",
    "    def program(self, percepts):\n",
    "        '''Reads the percept and returns the action'''\n",
    "        golds = [p for p in percepts if isinstance(p[0], Gold)]\n",
    "\n",
    "        if len(golds) < 1:\n",
    "            r = self.location[0]\n",
    "            c = self.location[1]\n",
    "            \n",
    "            if self.direction == 'U' and r > 1:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'R' and c < 3:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'D' and r < 3:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'L' and c > 1:\n",
    "                return 'ADVANCE'\n",
    "\n",
    "            return 'TURN'\n",
    "        \n",
    "\n",
    "        state = golds[0]\n",
    "        rGold = state[2][0] \n",
    "        cGold = state[2][1]\n",
    "        rAgent = self.location[0]\n",
    "        cAgent = self.location[1]\n",
    "\n",
    "        # If there is gold in the current location, stay\n",
    "        if cGold == cAgent and rGold == rAgent:\n",
    "            return 'STAY'\n",
    "        \n",
    "        # TODO: Agregar que si esta en linea con la dirección, se mueva adelante\n",
    "\n",
    "        # Check if the percepted gold is in front of the agent\n",
    "        if self.direction == 'U' and rGold < rAgent and cGold >= cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'R' and rGold >= rAgent and cGold > cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'D' and rGold > rAgent and cGold <= cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'L' and rGold <= rAgent and cGold < cAgent:\n",
    "            return 'ADVANCE'\n",
    "\n",
    "        return 'TURN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent(Agent):\n",
    "    '''\n",
    "    Initializes the Agent's variables. Every Agent starts being alive with a performance of 100 and facing the right direction.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.location = (0,0)\n",
    "        self.direction = 'R'\n",
    "        self.performance = 100\n",
    "        self.alive = True\n",
    "\n",
    "        '''Initialized the Agent's internal state which will be more helpful in case of a Partially Observable Enrivonment'''\n",
    "        self.internal_state = [[{'Visited': -1, 'Gold': -1, 'Traps': -1} for row in range(5)] for col in range(5)]\n",
    "    \n",
    "    def __str__(self):\n",
    "        '''Prints the internal state of the agent'''\n",
    "        facing = {'U': 'UP', 'R': 'RIGHT', 'D': 'DOWN', 'L': 'LEFT'}\n",
    "        return \"(%s, %s, %s)\" % (self.location[0], self.location[1], facing[self.direction])\n",
    "    \n",
    "    def print_internal_state(self):\n",
    "        '''\n",
    "        Show the actual internal state of the Agent, with information about the visited cells, gold and traps discovered by previous steps.\n",
    "        '''\n",
    "        print('Agent internal state:')\n",
    "        print('     0       1       2       3       4')\n",
    "        print('  (A G T) (A G T) (A G T) (A G T) (A G T)')\n",
    "        for r in range(len(self.internal_state)):\n",
    "            row = '%s ' % r\n",
    "            row2 = ''\n",
    "            for c in range(len(self.internal_state[r])):\n",
    "                visited = '?' if self.internal_state[r][c]['Visited'] < 0 else '-' if self.internal_state[r][c]['Visited'] == 0 else 'V'\n",
    "                golds = '?' if self.internal_state[r][c]['Gold'] < 0 else '-' if self.internal_state[r][c]['Gold'] == 0 else self.internal_state[r][c]['Gold']\n",
    "                traps = '?' if self.internal_state[r][c]['Traps'] < 0 else '-' if self.internal_state[r][c]['Traps'] == 0 else self.internal_state[r][c]['Traps']\n",
    "                \n",
    "                row += '(%s %s %s) ' % (visited, golds, traps)\n",
    "                row2 +='(%s %s %s) ' % (self.internal_state[r][c]['Visited'], self.internal_state[r][c]['Gold'], self.internal_state[r][c]['Traps'])\n",
    "            print(row)\n",
    "\n",
    "    def print_percepts(self, percept, radius):\n",
    "        '''\n",
    "        Receives a list of percepts around the player and show them in a grid form.\n",
    "        '''\n",
    "        print(\"Percept:\")\n",
    "        \n",
    "        x, y = self.location\n",
    "\n",
    "        row = '  '\n",
    "        for c in range(-radius, radius+1):\n",
    "            if self.is_inbounds((0, y+c)):\n",
    "                row += '   %s    ' % (y+c)                    \n",
    "        print(row)\n",
    "\n",
    "        for r in range(-radius, radius+1):\n",
    "            if self.is_inbounds((x+r, 0)):\n",
    "                row = '%s ' % (x+r)\n",
    "                for c in range(-radius, radius+1):\n",
    "                    if self.is_inbounds((x+r, y+c)):\n",
    "                        agentState = '-' if x != x+r or y != y+c else self.direction\n",
    "\n",
    "                        cellGold = self.internal_state[x+r][y+c]['Gold']\n",
    "                        gold = '-' if cellGold < 1 else cellGold\n",
    "\n",
    "                        cellTraps = self.internal_state[x+r][y+c]['Traps']\n",
    "                        traps = '-' if cellTraps < 1 else cellTraps\n",
    "\n",
    "                        row += '(%s %s %s) ' % (agentState, gold, traps)\n",
    "                    \n",
    "                print(row)\n",
    "        print('')\n",
    "    \n",
    "    '''Action when the agent enters a cell with a gold'''\n",
    "    def get_gold(self):\n",
    "        '''Gives 10 point from the agent'''\n",
    "        self.performance += 10\n",
    "\n",
    "    '''Action when the agent falls into a trap'''\n",
    "    def fall_in_trap(self):\n",
    "        '''Takes 5 points from the agent'''\n",
    "        self.performance -= 5\n",
    "\n",
    "    '''State when the agent recieve the action to turn'''\n",
    "    def turn_clockwise(self):\n",
    "        '''Reduces 1 point from the agent'''\n",
    "        self.performance -= 1\n",
    "        \n",
    "        '''Turns its direction clockwise'''\n",
    "        turns = {'U': 'R', 'R': 'D', 'D': 'L', 'L': 'U'}\n",
    "        self.direction = turns[self.direction]\n",
    "\n",
    "    '''State when the agent recieve the action to advance'''\n",
    "    def move_forward(self):\n",
    "        '''Reduces 1 point from the agent'''\n",
    "        self.performance -= 1\n",
    "\n",
    "        '''Advances in the direction of the agent'''\n",
    "        '''And checks that the agents does not move outside the grid'''\n",
    "        r = self.location[0]\n",
    "        c = self.location[1]\n",
    "        if self.direction == \"U\" and r != 0:\n",
    "            r = r-1\n",
    "        elif self.direction == \"R\" and c != 4:\n",
    "            c = c+1\n",
    "        elif self.direction == \"D\" and r != 4:\n",
    "            r = r+1\n",
    "        elif self.direction == \"L\" and c != 0:\n",
    "            c = c-1\n",
    "        self.location = (r, c)\n",
    "\n",
    "        '''Reduces the performance if the agent enters a visited cell'''\n",
    "        if self.internal_state[r][c]['Visited'] == 1:\n",
    "            self.performance -= 2\n",
    "\n",
    "    def is_inbounds(self, location):\n",
    "        '''Checks to make sure that the location is inbounds (within walls if we have walls)'''\n",
    "        x,y = location\n",
    "        return not (x < 0 or x >= 5 or y < 0 or y >= 5)\n",
    "    \n",
    "    # Update the internal state of the agent, with the new information after executing an action\n",
    "    def update_internal_state(self, percepts, radius=4):\n",
    "        \n",
    "        x, y = self.location\n",
    "        near_locations = []\n",
    "\n",
    "        # Find the locations where the agent need to update its internal state\n",
    "        for r in range(-radius, radius+1):\n",
    "            for c in range(-radius, radius+1):\n",
    "                near_locations.append((x+r, y+c))\n",
    "\n",
    "        # Delete internal state in percept radious\n",
    "        for loc in near_locations:\n",
    "            if self.is_inbounds(loc):\n",
    "                cell = self.internal_state[loc[0]][loc[1]]\n",
    "                cell['Gold'] = 0\n",
    "                cell['Traps'] = 0\n",
    "\n",
    "                if cell['Visited'] == -1:\n",
    "                    cell['Visited'] = 0\n",
    "        \n",
    "        # Update internal state in percept radious\n",
    "        for percept in percepts:\n",
    "            xPercept, yPercept = percept[2]\n",
    "            if isinstance(percept[0], Trap):\n",
    "                self.internal_state[xPercept][yPercept]['Traps'] += 1\n",
    "            if isinstance(percept[0], Gold):\n",
    "                self.internal_state[xPercept][yPercept]['Gold'] += 1\n",
    "        \n",
    "        # Mark the current agent location as visited\n",
    "        self.internal_state[x][y]['Visited'] = 1\n",
    "    \n",
    "    def program(self, percepts):\n",
    "        '''Reads the percept and returns the action'''\n",
    "\n",
    "        golds = [p for p in percepts if isinstance(p[0], Gold)]\n",
    "\n",
    "        # If there is no gold, look in the internal state for gold\n",
    "        if len(golds) < 1:\n",
    "            golds = []\n",
    "            for row in range(len(self.internal_state)):\n",
    "                for column in range(len(self.internal_state[0])):\n",
    "                    if self.internal_state[row][column]['Gold'] > 0:\n",
    "                        distance = distance_m((row, column), self.location)\n",
    "                        golds.append(((row,column), distance, (row,column)))\n",
    "            \n",
    "            # Sort gold by distance\n",
    "            golds = sorted(golds, key=lambda tup: tup[1])\n",
    "\n",
    "        # If there is still no gold found, take default behaviour\n",
    "        if len(golds) < 1:\n",
    "            r = self.location[0]\n",
    "            c = self.location[1]\n",
    "            \n",
    "            if self.direction == 'U' and r > 1:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'R' and c < 3:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'D' and r < 3:\n",
    "                return 'ADVANCE'\n",
    "            elif self.direction == 'L' and c > 1:\n",
    "                return 'ADVANCE'\n",
    "\n",
    "            return 'TURN'\n",
    "\n",
    "\n",
    "        state = golds[0]\n",
    "        rGold = state[2][0] \n",
    "        cGold = state[2][1]\n",
    "        rAgent = self.location[0]\n",
    "        cAgent = self.location[1]\n",
    "\n",
    "        # If there is gold in the current location, stay\n",
    "        if cGold == cAgent and rGold == rAgent:\n",
    "            return 'STAY'\n",
    "        \n",
    "        # TODO: Agregar que si esta en linea con la dirección, se mueva adelante\n",
    "\n",
    "        # Check if the percepted gold is in front of the agent\n",
    "        if self.direction == 'U' and rGold < rAgent and cGold >= cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'R' and rGold >= rAgent and cGold > cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'D' and rGold > rAgent and cGold <= cAgent:\n",
    "            return 'ADVANCE'\n",
    "        elif self.direction == 'L' and rGold <= rAgent and cGold < cAgent:\n",
    "            return 'ADVANCE'\n",
    "\n",
    "        return 'TURN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Observable Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This class is a representation of a Fully Observable Environment which let\n",
    "the Agents' percepts have complete access to the multidimensional states of\n",
    "the task environment.\n",
    "'''\n",
    "class FullyObservableEnvironment(Environment):\n",
    "    '''Creates a FullyObservableEnvironment as a subclass of Environment'''\n",
    "    def __init__(self, width=5, height=5, radius=4):\n",
    "        super(FullyObservableEnvironment, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.radius_of_vision = radius\n",
    "\n",
    "    def __str__(self):\n",
    "        grid = self.get_grid()\n",
    "        gridStr = []\n",
    "        gridStr.append('     0       1       2       3       4')\n",
    "        gridStr.append('  (A G T) (A G T) (A G T) (A G T) (A G T)')\n",
    "        for r in range(self.width):\n",
    "            row = ''\n",
    "            row += str(r)+' '\n",
    "            for c in range(self.height):\n",
    "                gold = 0\n",
    "                traps = 0\n",
    "                agent = None\n",
    "                for obj in grid[r][c]:\n",
    "                    if isinstance(obj, Agent):\n",
    "                        agent = obj\n",
    "                    elif isinstance(obj, Gold):\n",
    "                        gold += 1\n",
    "                    elif isinstance(obj, Trap):\n",
    "                        traps += 1\n",
    "                cell = '(%s %s %s) ' % ('-' if agent is None else agent.direction, '-' if gold < 1 else gold, '-' if traps < 1 else traps)\n",
    "                row += cell\n",
    "            row += '\\n'\n",
    "            gridStr.append(row)\n",
    "        return '\\n'.join(gridStr)\n",
    "\n",
    "    def run(self, steps=1000):\n",
    "        \"Run the Environment for given number of time steps.\"\n",
    "        print('---------------------------')\n",
    "        print('Initial State')\n",
    "        print('---------------------------')\n",
    "        \n",
    "        print(self)\n",
    "\n",
    "        for agent in self.agents:\n",
    "            percepts = self.percept(agent)\n",
    "\n",
    "            if isinstance(agent, ModelBasedAgent):\n",
    "                agent.update_internal_state(percepts, self.radius_of_vision)\n",
    "\n",
    "            self.print_agent_percept(agent, percepts)\n",
    "\n",
    "            if isinstance(agent, ModelBasedAgent):\n",
    "                agent.print_internal_state()\n",
    "                print('')\n",
    "\n",
    "            print(\"Agent state: %s\" % agent)\n",
    "            print(\"Agent performance: %s\" % agent.performance)\n",
    "            print('')\n",
    "        \n",
    "        \n",
    "\n",
    "        print('---------------------------')\n",
    "        print('Run details')\n",
    "        print('---------------------------')\n",
    "        i = 1\n",
    "        for step in range(steps):\n",
    "            if self.is_done():\n",
    "                return\n",
    "            print('<STEP %s>' % i)\n",
    "            self.step()\n",
    "            i += 1\n",
    "    \n",
    "    def get_grid(self):\n",
    "        grid =[]\n",
    "        for x in range(self.width):\n",
    "            row = []\n",
    "            for y in range(self.height):\n",
    "                row.append(self.list_things_at((x, y)))\n",
    "            grid.append(row)\n",
    "        return grid\n",
    "\n",
    "    def default_location(self, thing):\n",
    "        return (random.randint(0, self.width-1), random.randint(0, self.width-1))\n",
    "\n",
    "    def is_done(self):\n",
    "        golds = [g for g in self.things if isinstance(g, Gold)]\n",
    "\n",
    "        if not golds:\n",
    "            return True\n",
    "\n",
    "        return False         \n",
    "\n",
    "    def percept(self, agent):\n",
    "        things_list = [(thing, distance_m(agent.location, thing.location), thing.location) for thing in self.things if not isinstance(thing, Agent)]\n",
    "        sorted_things = sorted(things_list, key=lambda tup: tup[1])\n",
    "        return sorted_things\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        '''changes the state of the environment based on what the agent does.'''\n",
    "\n",
    "        if action == \"TURN\":\n",
    "            agent.turn_clockwise()\n",
    "        elif action == \"ADVANCE\":\n",
    "            agent.move_forward()\n",
    "        elif action == \"STAY\":\n",
    "            pass\n",
    "                \n",
    "        things = self.list_things_at(agent.location)\n",
    "        golds = [g for g in things if isinstance(g, Gold)]\n",
    "        traps = [t for t in things if isinstance(t, Trap)]\n",
    "\n",
    "        if golds:\n",
    "            agent.get_gold()\n",
    "            self.delete_thing(golds[0])\n",
    "                \n",
    "        if traps:\n",
    "            agent.fall_in_trap()\n",
    "            self.delete_thing(traps[0])\n",
    "\n",
    "        print(\"SELECTED ACTION: \", action)\n",
    "        print(\"Agent state: \", agent)\n",
    "        print(\"Agent performance: %s\" % agent.performance)\n",
    "\n",
    "        print('\\nEnvironment: ')\n",
    "        print(self)\n",
    "\n",
    "        percepts = self.percept(agent)\n",
    "\n",
    "        if isinstance(agent, ModelBasedAgent):\n",
    "            agent.update_internal_state(percepts, self.radius_of_vision)\n",
    "\n",
    "        self.print_agent_percept(agent, percepts)\n",
    "\n",
    "        if isinstance(agent, ModelBasedAgent):\n",
    "            agent.print_internal_state()\n",
    "            print('')\n",
    "\n",
    "    def print_agent_percept(self, agent, percepts):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Observable Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This class is a representation of a Partially Observable Environment\n",
    "where some of the information required for optimal decision making is\n",
    "hidden until it emerges due to Agents' level activity.\n",
    "'''\n",
    "class PartiallyObservableEnvironment(FullyObservableEnvironment):\n",
    "    '''Creates a PartiallyObservableEnvironment as a subclass of FullyObservableEnvironment,\n",
    "    they have the same function but the Partially Observable one is limited to a radius of information\n",
    "    around the agent.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        FullyObservableEnvironment.__init__(self)\n",
    "        self.radius_of_vision = 1\n",
    "\n",
    "    def percept(self, agent):\n",
    "        '''Only perceive near things'''\n",
    "        x, y = agent.location\n",
    "        near_locations = [(x-1, y-1), (x-1, y), (x-1, y+1), (x, y-1), (x, y), (x, y+1), (x+1, y-1), (x+1, y), (x+1, y+1)]\n",
    "        things_near = []\n",
    "\n",
    "        for loc in near_locations:\n",
    "            things_near += self.percepts_from(agent, loc)\n",
    "      \n",
    "        return things_near\n",
    "    \n",
    "    def percepts_from(self, agent, location, tclass=Thing):\n",
    "        ''' Get percepts from a defined location'''\n",
    "        things = self.list_things_at((location[0], location[1]))\n",
    "        percepts = [(p, distance_m(agent.location, p.location), p.location) for p in things if not isinstance(p, Agent)]\n",
    "        return percepts\n",
    "\n",
    "    def print_agent_percept(self, agent, percepts):\n",
    "        agent.print_percepts(percepts, self.radius_of_vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize all the libraries which are necessary.\n",
    "\n",
    "Some of the classes imported are the following:\n",
    "<ul>\n",
    "    <li><b>FullyObservableEnvironment:</b></li>\n",
    "        &emsp; This class contains the type of environment where you can perceive all places where there are Golds and Traps. All other relevant portions of the environment are also visible.\n",
    "    <li><b>PartiallyObservableEnvironment:</b></li>\n",
    "        &emsp; In this type of environment some states are hidden, that means, the agent(s) can never see the entire state of the environment. This kind of environment needs agents with memory to be solved.\n",
    "    <li><b>ReflexAgent:</b></li>\n",
    "        &emsp; This class implements the Simple Reflex Agents which acts only on basis of the percepts that the agents receives from the environment. It's actions are based on condition-action rules.\n",
    "    <li><b>ModelBasedAgent:</b></li>\n",
    "        &emsp; This is the kind of agents which maintains the structure that describes the part of the world which cannot see. This knowledge is what is called model of the world.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Partially Observable Environment Tests</b></h1>\n",
    "\n",
    "The first Agent to be tested is the Reflex Agent in a Partially Observable Environment.\n",
    "In addition to the agent, we also add 5 pieces of gold and 6 traps in specified positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (R - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n1 (- - 1) (- - -) \n2 (R - -) (- - -) \n3 (- - -) (- - 3) \n\nAgent state: (2, 0, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (R - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n1 (- - 1) (- - -) (- - -) \n2 (- - -) (R - -) (- - -) \n3 (- - -) (- - 3) (- - -) \n\n<STEP 2>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 2, RIGHT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (R - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     1       2       3    \n1 (- - -) (- - -) (- - -) \n2 (- - -) (R - -) (- 1 1) \n3 (- - 3) (- - -) (- - -) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n1 (- - -) (- - -) (- 2 -) \n2 (- - -) (R - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, DOWN)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n1 (- - -) (- - -) (- 2 -) \n2 (- - -) (D - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\n<STEP 5>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, LEFT)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n1 (- - -) (- - -) (- 2 -) \n2 (- - -) (L - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n1 (- - -) (- - -) (- 2 -) \n2 (- - -) (U - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (U - -) (- 2 -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (R - -) (- 2 -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R 1 -) \n2 (- - -) (- - -) \n\n<STEP 10>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R - -) \n2 (- - -) (- - -) \n\n<STEP 11>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (D - -) \n2 (- - -) (- - -) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n1 (- - -) (- - -) \n2 (- - -) (D - -) \n3 (- - -) (- - -) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (D - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n2 (- - -) (- - -) \n3 (- - -) (D - -) \n4 (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, LEFT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (L - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n2 (- - -) (- - -) \n3 (- - -) (L - -) \n4 (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (L - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - -) (- - -) \n4 (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 110\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (L - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - 3) (L - -) (- - -) \n4 (- - -) (- - -) (- - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (L - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - 2) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (U - 1) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (U - 1) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, RIGHT)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (R - -) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\n<STEP 20>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, DOWN)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (D - -) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- 1 -) (D - -) (- - -) \n\n<STEP 22>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- 1 -) (L - -) (- - -) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n3 (- - -) (- - -) \n4 (L - -) (- - -) \n\n<STEP 24>\nSELECTED ACTION:  TURN\nAgent state:  (4, 0, UP)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n3 (- - -) (- - -) \n4 (U - -) (- - -) \n\n<STEP 25>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, UP)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n2 (- - -) (- - -) \n3 (U - -) (- - -) \n4 (- - -) (- - -) \n\n<STEP 26>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n1 (- - 1) (- - -) \n2 (U - -) (- - -) \n3 (- - -) (- - -) \n\n<STEP 27>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n0 (- - -) (- 1 -) \n1 (U - -) (- - -) \n2 (- - -) (- - -) \n\n<STEP 28>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 86\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n0 (U - -) (- 1 -) \n1 (- - -) (- - -) \n\n<STEP 29>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 85\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n0 (R - -) (- 1 -) \n1 (- - -) (- - -) \n\n<STEP 30>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n0 (- - -) (R - -) (- - -) \n1 (- - -) (- - -) (- - -) \n\n"
    }
   ],
   "source": [
    "environment = PartiallyObservableEnvironment()\n",
    "\n",
    "reflex_agent = ReflexAgent()\n",
    "environment.add_thing(reflex_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second agent in the Partially Observable Environment is the Model Based Agent which will be tested with gold and traps at the same positions as the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     2       3       4    \n1 (- - -) (- - -) (- 2 -) \n2 (- - -) (U - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 6>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     2       3       4    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (U - -) (- 2 -) \n2 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (- 2 -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 7>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     2       3       4    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (R - -) (- 2 -) \n2 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (- 2 -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R 1 -) \n2 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V 1 -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 9>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R - -) \n2 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (D - -) \n2 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (- - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n1 (- - -) (- - -) \n2 (- - -) (D - -) \n3 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (D - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n2 (- - -) (- - -) \n3 (- - -) (D - -) \n4 (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (- - 3) (- - -) (- - -) (V - -) \n4 (? ? ?) (? ? ?) (? ? ?) (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, LEFT)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (L - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     3       4    \n2 (- - -) (- - -) \n3 (- - -) (L - -) \n4 (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (- - 3) (- - -) (- - -) (V - -) \n4 (? ? ?) (? ? ?) (? ? ?) (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, LEFT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (L - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     2       3       4    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - -) (- - -) \n4 (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (- - 3) (- - -) (V - -) (V - -) \n4 (? ? ?) (? ? ?) (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (L - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - 3) (L - -) (- - -) \n4 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (- - 3) (V - -) (V - -) (V - -) \n4 (? ? ?) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (L - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - 2) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - 2) (V - -) (V - -) (V - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (U - 1) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (U - 1) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - 1) (V - -) (V - -) (V - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, RIGHT)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (R - -) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, DOWN)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (D - -) (- - -) \n4 (- 1 -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- 1 -) (D - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- 1 -) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n3 (- - -) (- - -) \n4 (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 23>\nSELECTED ACTION:  TURN\nAgent state:  (4, 0, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n3 (- - -) (- - -) \n4 (U - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (- - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 24>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, UP)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n2 (- - -) (- - -) \n3 (U - -) (- - -) \n4 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (- - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 25>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n1 (- - 1) (- - -) \n2 (U - -) (- - -) \n3 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 26>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n0 (- - -) (- 1 -) \n1 (U - -) (- - -) \n2 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 27>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n0 (U - -) (- 1 -) \n1 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 28>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 88\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1    \n0 (R - -) (- 1 -) \n1 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 29>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept:\n     0       1       2    \n0 (- - -) (R - -) (- - -) \n1 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (V - -) (V - -) (V - -) (V - -) \n3 (V - -) (V - -) (V - -) (V - -) (V - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = PartiallyObservableEnvironment()\n",
    "\n",
    "model_agent = ModelBasedAgent()\n",
    "environment.add_thing(model_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the implementation of the agents in the Partially Observable Environment we see the results of the <u>Reflex Agent's</u> performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "94"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "reflex_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the <u>Model-Based Agent</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "97"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model_agent.performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Fully Observable Environment Tests</b></h1>\n",
    "\n",
    "In this second part of the homework we use the Fully Observable Environment, first with the Reflex Agent inside it, as well as the past exercise, we use gold and traps in explicit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (R 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent state: (2, 3, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  STAY\nAgent state:  (2, 3, RIGHT)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, DOWN)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 3>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, LEFT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, UP)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 8>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 119\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, LEFT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (L - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, LEFT)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (L - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 2, LEFT)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (L - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, LEFT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (L - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, UP)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (U - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, UP)\nAgent performance: 120\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  TURN\nAgent state:  (0, 1, RIGHT)\nAgent performance: 119\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (0, 1, DOWN)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (D - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, DOWN)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = FullyObservableEnvironment()\n",
    "\n",
    "reflex_agent = ReflexAgent()\n",
    "environment.add_thing(reflex_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Model Based Agent in the Fully Observable Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (R - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 3) (- - -) (- - -) (- - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent state: (3, 1, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, DOWN)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (D - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 2) (- - -) (- - -) (- - -) \n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 2>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 2) (- - -) (- - -) (- - -) \n4 (- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 3>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 2) (- - -) (- - -) (- - -) \n4 (- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 4>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 5>\nSELECTED ACTION:  TURN\nAgent state:  (4, 0, UP)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (- - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 6>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (U - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (U - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (- - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 2, RIGHT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (R - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (- - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 3, RIGHT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (0, 3, DOWN)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (- - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, DOWN)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (D - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (- 1 1) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, LEFT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, UP)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (- 2 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V 1 -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - -) \n1 (V - -) (- - -) (- - -) (V - -) (V - -) \n2 (V - -) (- - -) (- - -) (V - -) (- - -) \n3 (V - -) (V - 2) (- - -) (- - -) (- - -) \n4 (V - -) (V - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = FullyObservableEnvironment()\n",
    "\n",
    "model_agent = ModelBasedAgent()\n",
    "environment.add_thing(model_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the performance of the <u>Reflex Agent</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "115"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "reflex_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the <u>Model Based Agent</u> in the Fully Observable Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "112"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Additional tests:</h1>\n",
    "In addition to the tests performed previously, we can run several times the <u>Reflex Agent</u> in the <u>Partially Observable Environment with gold and traps placed at random positions...</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "      2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (R - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- 1 1) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     1       2       3    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (R - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, RIGHT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- 1 1) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n0 (- - -) (- - -) (- - -) \n1 (- - -) (R - -) (- - -) \n2 (- - -) (- - -) (- 1 1) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- 1 1) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R - -) \n2 (- - -) (- 1 1) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 88\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- 1 1) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (D - -) \n2 (- - -) (- 1 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n1 (- - -) (- - -) \n2 (- - -) (D - -) \n3 (- 1 -) (- - -) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (D - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n2 (- - -) (- - -) \n3 (- 1 -) (D - -) \n4 (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, LEFT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- 1 -) (- - -) (- 1 2) (- 1 -) (L - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     3       4    \n2 (- - -) (- - -) \n3 (- 1 -) (L - -) \n4 (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, LEFT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- 1 -) (- - -) (- 1 2) (L - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     2       3       4    \n2 (- - -) (- - -) (- - -) \n3 (- 1 2) (L - -) (- - -) \n4 (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- 1 -) (- - -) (L - 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - 1) (- - -) \n4 (- - -) (- - -) (- - -) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- 1 -) (L - -) (- - 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- 1 -) (L - -) (- - 1) \n4 (- - -) (- - -) (- - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (L - -) (- - -) (- - 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n     0       1    \n2 (- - -) (- - -) \n3 (L - -) (- - -) \n4 (- - -) (- - -) \n\n---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (R - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- 1 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     2       3       4    \n0 (- - 1) (- - -) (- - -) \n1 (- - -) (R - 1) (- - -) \n2 (- - -) (- - -) (- - -) \n\nAgent state: (1, 3, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, DOWN)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (D - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- 1 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     2       3       4    \n0 (- - 1) (- - -) (- - -) \n1 (- - -) (D - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 2>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - -) (- 1 1) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     2       3       4    \n1 (- - -) (- - -) (- - -) \n2 (- - -) (D - -) (- - -) \n3 (- 1 1) (- - -) (- - -) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, DOWN)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- 1 1) (D - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     2       3       4    \n2 (- - -) (- - -) (- - -) \n3 (- 1 1) (D - -) (- - -) \n4 (- 1 -) (- 1 -) (- - -) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (3, 3, LEFT)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- 1 1) (L - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     2       3       4    \n2 (- - -) (- - -) (- - -) \n3 (- 1 1) (L - -) (- - -) \n4 (- 1 -) (- 1 -) (- - -) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (L - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (L - -) (- - -) \n4 (- - -) (- 1 -) (- 1 -) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (3, 2, UP)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (U - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (U - -) (- - -) \n4 (- - -) (- 1 -) (- 1 -) \n\n<STEP 7>\nSELECTED ACTION:  TURN\nAgent state:  (3, 2, RIGHT)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (R - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (R - -) (- - -) \n4 (- - -) (- 1 -) (- 1 -) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (3, 2, DOWN)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (D - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- 1 -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (D - -) (- - -) \n4 (- - -) (- 1 -) (- 1 -) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 2, DOWN)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (D - -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (D - -) (- 1 -) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (4, 2, LEFT)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (L - -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (L - -) (- 1 -) \n\n<STEP 11>\nSELECTED ACTION:  TURN\nAgent state:  (4, 2, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (U - -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (U - -) (- 1 -) \n\n<STEP 12>\nSELECTED ACTION:  TURN\nAgent state:  (4, 2, RIGHT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (R - -) (- 1 -) (- - -) \n\nPercept\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (R - -) (- 1 -) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 3, RIGHT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (R - -) (- - -) \n\nPercept\n     2       3       4    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (R - -) (- - -) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (4, 3, DOWN)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (D - -) (- - -) \n\nPercept\n     2       3       4    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (D - -) (- - -) \n\n<STEP 15>\nSELECTED ACTION:  TURN\nAgent state:  (4, 3, LEFT)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (L - -) (- - -) \n\nPercept\n     2       3       4    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (L - -) (- - -) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 2, LEFT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (L - -) (- - -) (- - -) \n\nPercept\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (L - -) (- - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, LEFT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (L - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (L - -) (- - -) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, UP)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (U - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (U - -) (- - -) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n2 (- - -) (- - -) (- - -) \n3 (- - -) (U - -) (- - -) \n4 (- - -) (- - -) (- - -) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n2 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n1 (- - -) (- - 1) (- - -) \n2 (- - -) (U - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, UP)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n0 (- 2 2) (- - -) (- - 1) \n1 (- - -) (U - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 22>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, RIGHT)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n0 (- 2 2) (- - -) (- - 1) \n1 (- - -) (R - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 23>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, DOWN)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n0 (- 2 2) (- - -) (- - 1) \n1 (- - -) (D - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 24>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, LEFT)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (L - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1       2    \n0 (- 2 2) (- - -) (- - 1) \n1 (- - -) (L - -) (- - -) \n2 (- - -) (- - -) (- - -) \n\n<STEP 25>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, LEFT)\nAgent performance: 88\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (L - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1    \n0 (- 2 2) (- - -) \n1 (L - -) (- - -) \n2 (- - -) (- - -) \n\n<STEP 26>\nSELECTED ACTION:  TURN\nAgent state:  (1, 0, UP)\nAgent performance: 87\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 2) (- - -) (- - 1) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1    \n0 (- 2 2) (- - -) \n1 (U - -) (- - -) \n2 (- - -) (- - -) \n\n<STEP 27>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 89\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U 1 1) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1    \n0 (U 1 1) (- - -) \n1 (- - -) (- - -) \n\n<STEP 28>\nSELECTED ACTION:  STAY\nAgent state:  (0, 0, UP)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept\n     0       1    \n0 (U - -) (- - -) \n1 (- - -) (- - -) \n\n"
    }
   ],
   "source": [
    "numberOfTests = 5\n",
    "totalFitness_PartiallyObservableReflex = 0\n",
    "fitness_PartiallyObservableReflex = []\n",
    "for _ in range(numberOfTests):\n",
    "    environment = PartiallyObservableEnvironment()\n",
    "\n",
    "    reflex_agent = ReflexAgent()\n",
    "    environment.add_thing(reflex_agent)\n",
    "\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "\n",
    "    environment.run()\n",
    "\n",
    "    totalFitness_PartiallyObservableReflex += reflex_agent.performance\n",
    "    fitness_PartiallyObservableReflex.append(reflex_agent.performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... as well as the <u>Model-Based Agent</u> in the same kind of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " - -) (- 1 2) \n\nPercept:\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (R - -) \n2 (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n1 (- - -) (V - -) (V - -) (V - -) (V - -) \n2 (V - -) (V - -) (- - 1) (- - -) (- 1 -) \n3 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n4 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- 1 -) \n\n3 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- 1 2) \n\nPercept:\n     3       4    \n0 (- - -) (- - -) \n1 (- - -) (D - -) \n2 (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n1 (- - -) (V - -) (V - -) (V - -) (V - -) \n2 (V - -) (V - -) (- - 1) (- - -) (- 1 -) \n3 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n4 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (D - -) \n\n3 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- 1 2) \n\nPercept:\n     3       4    \n1 (- - -) (- - -) \n2 (- - -) (D - -) \n3 (- - 1) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n1 (- - -) (V - -) (V - -) (V - -) (V - -) \n2 (V - -) (V - -) (- - 1) (- - -) (V - -) \n3 (V - -) (V - -) (- - -) (- - 1) (- - -) \n4 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - 1) (D - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- 1 2) \n\nPercept:\n     3       4    \n2 (- - -) (- - -) \n3 (- - 1) (D - -) \n4 (- - -) (- 1 2) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n1 (- - -) (V - -) (V - -) (V - -) (V - -) \n2 (V - -) (V - -) (- - 1) (- - -) (V - -) \n3 (V - -) (V - -) (- - -) (- - 1) (V - -) \n4 (- - -) (- - -) (- - -) (- - -) (- 1 2) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 4, DOWN)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (D - 1) \n\nPercept:\n     3       4    \n3 (- - 1) (- - -) \n4 (- - -) (D - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - 1) (- - -) (- - -) \n1 (- - -) (V - -) (V - -) (V - -) (V - -) \n2 (V - -) (V - -) (- - 1) (- - -) (V - -) \n3 (V - -) (V - -) (- - -) (- - 1) (V - -) \n4 (- - -) (- - -) (- - -) (- - -) (V - 1) \n\n---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- 1 -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (R 1 -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1       2    \n1 (- 1 -) (- - -) (- - -) \n2 (- - -) (R 1 -) (- - -) \n3 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- 1 -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (- - -) (V 1 -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\nAgent state: (2, 1, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, DOWN)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- 1 -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (D - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1       2    \n1 (- 1 -) (- - -) (- - -) \n2 (- - -) (D - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- 1 -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (- - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, LEFT)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- 1 -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (L - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1       2    \n1 (- 1 -) (- - -) (- - -) \n2 (- - -) (L - -) (- - -) \n3 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- 1 -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (- - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, LEFT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- 1 -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1    \n1 (- 1 -) (- - -) \n2 (L - -) (- - -) \n3 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- 1 -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, UP)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- 1 -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1    \n1 (- 1 -) (- - -) \n2 (U - -) (- - -) \n3 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n1 (- 1 -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (U - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1    \n0 (- - 1) (- 1 -) \n1 (U - -) (- - -) \n2 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - 1) (- 1 -) (? ? ?) (? ? ?) (? ? ?) \n1 (V - -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 6>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1    \n0 (U - -) (- 1 -) \n1 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (? ? ?) (? ? ?) (? ? ?) \n1 (V - -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 7>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1    \n0 (R - -) (- 1 -) \n1 (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- 1 -) (? ? ?) (? ? ?) (? ? ?) \n1 (V - -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     0       1       2    \n0 (- - -) (R - -) (- - -) \n1 (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n1 (V - -) (- - -) (- - -) (? ? ?) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 2, RIGHT)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (R - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     1       2       3    \n0 (- - -) (R - -) (- - -) \n1 (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (- - -) (? ? ?) \n1 (V - -) (- - -) (- - -) (- - 1) (? ? ?) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 10>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 3, RIGHT)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (R - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     2       3       4    \n0 (- - -) (R - -) (- - 1) \n1 (- - -) (- - 1) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (- - 1) (- - -) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 11>\nSELECTED ACTION:  TURN\nAgent state:  (0, 3, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (D - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     2       3       4    \n0 (- - -) (D - -) (- - 1) \n1 (- - -) (- - 1) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (- - 1) (- - -) \n2 (V - -) (V - -) (- - -) (? ? ?) (? ? ?) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, DOWN)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     2       3       4    \n0 (- - -) (- - -) (- - 1) \n1 (- - -) (D - -) (- - -) \n2 (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (- - -) (- - 1) \n3 (- - -) (- - -) (- - -) (? ? ?) (? ? ?) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     2       3       4    \n1 (- - -) (- - -) (- - -) \n2 (- - -) (D - -) (- - 1) \n3 (- - -) (- - -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n4 (? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, DOWN)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (D - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (- - 1) (- - -) \n\nPercept:\n     2       3       4    \n2 (- - -) (- - -) (- - 1) \n3 (- - -) (D - -) (- - 1) \n4 (- 1 -) (- - 1) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (? ? ?) (? ? ?) (- 1 -) (- - 1) (- - -) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 3, DOWN)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (D - -) (- - -) \n\nPercept:\n     2       3       4    \n3 (- - -) (- - -) (- - 1) \n4 (- 1 -) (D - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (? ? ?) (? ? ?) (- 1 -) (V - -) (- - -) \n\n<STEP 16>\nSELECTED ACTION:  TURN\nAgent state:  (4, 3, LEFT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (- 1 -) (L - -) (- - -) \n\nPercept:\n     2       3       4    \n3 (- - -) (- - -) (- - 1) \n4 (- 1 -) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (? ? ?) (? ? ?) (- 1 -) (V - -) (- - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 2, LEFT)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (- - -) (L - -) (- - -) (- - -) \n\nPercept:\n     1       2       3    \n3 (- - -) (- - -) (- - -) \n4 (- - -) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (? ? ?) (- - -) (V - -) (V - -) (- - -) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, LEFT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - -) \n\nPercept:\n     0       1       2    \n3 (- - -) (- - -) (- - -) \n4 (- 1 -) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (- 1 -) (V - -) (V - -) (V - -) (- - -) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - -) \n\nPercept:\n     0       1    \n3 (- - -) (- - -) \n4 (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (V - -) (V - -) (V - -) (- - 1) \n1 (V - -) (- - -) (- - -) (V - -) (- - -) \n2 (V - -) (V - -) (- - -) (V - -) (- - 1) \n3 (- - -) (- - -) (- - -) (V - -) (- - 1) \n4 (V - -) (V - -) (V - -) (V - -) (- - -) \n\n"
    }
   ],
   "source": [
    "numberOfTests = 5\n",
    "totalFitness_PartiallyObservableModel = 0\n",
    "fitness_PartiallyObservableModel = []\n",
    "for _ in range(numberOfTests):\n",
    "    environment = PartiallyObservableEnvironment()\n",
    "\n",
    "    model_agent = ModelBasedAgent()\n",
    "    environment.add_thing(model_agent)\n",
    "\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "\n",
    "    environment.run()\n",
    "\n",
    "    totalFitness_PartiallyObservableModel += model_agent.performance\n",
    "    fitness_PartiallyObservableModel.append(model_agent.performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... to finally se their average performance. The maximum performance obtained by the <u>Reflex Agent</u> in the <u>Partially Observable Environment</u> was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "128"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "np.max(fitness_PartiallyObservableReflex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and for the <u>Model Based Agent</u>, its maximum performance was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "130"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "np.max(fitness_PartiallyObservableModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average, the <u>Reflex Agent</u> had a performance of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "111.0"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "totalFitness_PartiallyObservableReflex/numberOfTests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the <u>Model-Based Agent</u> had a performance of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "114.6"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "totalFitness_PartiallyObservableModel/numberOfTests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ") (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n4 (- - 1) (- - -) (- - -) (- - -) (- - 2) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 130\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n4 (- - 1) (- - -) (- - -) (- - -) (- - 2) \n\n---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- 1 -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (R - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\nAgent state: (3, 2, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (3, 2, DOWN)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- 1 -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (D - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (3, 2, LEFT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- 1 -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (L - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- 1 -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (L - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, UP)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- 1 -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (U - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, UP)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (U - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, RIGHT)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (R - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 7>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, DOWN)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (D - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, LEFT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (L - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (L - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, UP)\nAgent performance: 110\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 11>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, RIGHT)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (R - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 12>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, DOWN)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (D - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- 1 2) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, DOWN)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (D - 1) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (3, 0, LEFT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 15>\nSELECTED ACTION:  TURN\nAgent state:  (3, 0, UP)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 1 -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- - -) (- - -) (- - -) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 2, RIGHT)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (R - -) (- - -) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 3, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (R - -) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 4, RIGHT)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (R - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n4 (- - -) (- - -) (- - -) (- - 1) (- - -) \n\n---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - 1) (R - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\nAgent state: (2, 2, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (2, 2, DOWN)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - 1) (D - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (2, 2, LEFT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - 1) (L - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, LEFT)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (L - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, UP)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (U - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, UP)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 7>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, DOWN)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, LEFT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (L - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, LEFT)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (L - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (1, 0, UP)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- 2 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U 1 -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 12>\nSELECTED ACTION:  STAY\nAgent state:  (0, 0, UP)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, DOWN)\nAgent performance: 110\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (D - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, DOWN)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (D - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- 1 -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, DOWN)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (D - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, LEFT)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (L - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, UP)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (2, 0, RIGHT)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (R - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, RIGHT)\nAgent performance: 110\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, DOWN)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, DOWN)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (- 1 -) (- - 1) (- - 1) (- - 1) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - 1) (D - -) (- - 1) (- - 1) (- - 1) \n\n"
    }
   ],
   "source": [
    "numberOfTests = 5\n",
    "totalFitness_FullyObservableReflex = 0\n",
    "fitness_FullyObservableReflex = []\n",
    "for _ in range(numberOfTests):\n",
    "    environment = FullyObservableEnvironment()\n",
    "\n",
    "    reflex_agent = ReflexAgent()\n",
    "    environment.add_thing(reflex_agent)\n",
    "\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "\n",
    "    environment.run()\n",
    "\n",
    "    totalFitness_FullyObservableReflex += reflex_agent.performance\n",
    "    fitness_FullyObservableReflex.append(reflex_agent.performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (D - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - 1) (- 1 -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- - -) (- - -) (- - -) (- - -) \n1 (V - -) (V - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - 1) (- - -) (V - -) \n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- - -) (- - 1) (- 1 -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n4 (- - -) (- - -) (- - 1) (- 1 -) (- - 1) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- - -) (- - -) (- - -) (- - -) \n1 (V - -) (V - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - 1) (- - -) (V - -) \n3 (- - -) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- - -) (- - 1) (- 1 -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 4, DOWN)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - 1) (- 1 -) (D - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- - -) (- - -) (- - -) (- - -) \n1 (V - -) (V - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - 1) (- - -) (V - -) \n3 (- - -) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- - -) (- - 1) (- 1 -) (V - -) \n\n<STEP 16>\nSELECTED ACTION:  TURN\nAgent state:  (4, 4, LEFT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - 1) (- 1 -) (L - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- - -) (- - -) (- - -) (- - -) \n1 (V - -) (V - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - 1) (- - -) (V - -) \n3 (- - -) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- - -) (- - 1) (- 1 -) (V - -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 3, LEFT)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - 1) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - 1) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (V - -) (- - -) (- - -) (- - -) (- - -) \n1 (V - -) (V - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - 1) (- - -) (V - -) \n3 (- - -) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- - -) (- - 1) (V - -) (V - -) \n\n---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (R - -) (- - 1) (- - -) \n\n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (- - 1) (- - -) \n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent state: (1, 2, RIGHT)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, RIGHT)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, DOWN)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (- 1 -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 3>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, LEFT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 5>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, UP)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (R - -) (- 1 2) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (- 1 2) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, RIGHT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (R - 1) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - 1) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (2, 4, DOWN)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 9>\nSELECTED ACTION:  TURN\nAgent state:  (2, 4, LEFT)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (L - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (2, 4, UP)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (U - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (- - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, UP)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n\n1 (- - -) (- - -) (- - -) (- - -) (U - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- 1 -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 4, UP)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (U - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (0, 4, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (R - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (0, 4, DOWN)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (D - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, DOWN)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (- 1 -) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 4, DOWN)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (D - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (V - -) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (4, 4, LEFT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (- - -) (L - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- 1 -) (- - -) (- - -) (V - -) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 3, LEFT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (- - -) (L - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- 1 -) (- - -) (V - -) (V - -) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 2, LEFT)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- 1 -) (L - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (- 1 -) (V - -) (V - -) (V - -) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, LEFT)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (L - -) (- - -) (- - -) (- - -) \n\nAgent internal state:\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - 1) (- - -) (- - 1) (V - -) \n1 (- - -) (- - -) (V - -) (V - -) (V - -) \n2 (- - -) (- - -) (- - -) (V - -) (V - -) \n3 (- - 1) (- - -) (- - -) (- - -) (V - -) \n4 (- - -) (V - -) (V - -) (V - -) (V - -) \n\n"
    }
   ],
   "source": [
    "numberOfTests = 5\n",
    "totalFitness_FullyObservableModel = 0\n",
    "fitness_FullyObservableModel = []\n",
    "for _ in range(numberOfTests):\n",
    "    environment = FullyObservableEnvironment()\n",
    "\n",
    "    model_agent = ModelBasedAgent()\n",
    "    environment.add_thing(model_agent)\n",
    "\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "    gold = Gold()\n",
    "    environment.add_thing(gold)\n",
    "\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "    trap = Trap()\n",
    "    environment.add_thing(trap)\n",
    "\n",
    "    environment.run()\n",
    "\n",
    "    totalFitness_FullyObservableModel += model_agent.performance\n",
    "    fitness_FullyObservableModel.append(model_agent.performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum performance obtained by the <u>Reflex Agent</u> in the <u>Fully Observable Environment</u> was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "130"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "np.max(fitness_FullyObservableReflex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the max performance for all the runs for the <u>Model-Based Anges</u> in the same Environment was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "127"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "np.max(fitness_FullyObservableModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average, the performance of the <u>Reflex Agent</u> in the <u>Fully Observable Environment</u> was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "119.6"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "totalFitness_FullyObservableReflex/numberOfTests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the average for the <u>Model-Based Agent</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "117.4"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "totalFitness_FullyObservableModel/numberOfTests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Which agent behaves better in the Partially Observable Environment?:</b></li>\n",
    "        &emsp; During the tests that were carried out, we obtained better results in the vast majority of them using the Model-Based Agent which makes sense since it may not receive the full state of the environment and may not be able to see the gold pieces it is looking for, but it keep in his model some of the gold pieces already seen by it's percepts. In the case of the Reflex Agent, when it doesn't perceive any piece of gold it must explore the world which may lead to falling into traps.\n",
    "    <li><b>Which agent behaves better in the Fully Observable Environment?:</b></li>\n",
    "        &emsp; Using this kind of environment, both Agents had similar results because they didn't have to look for pieces of gold, their perceives always had the exact position of each gold in the Environment.\n",
    "    <li><b>Are the Agents behaving rationally?:</b></li>\n",
    "        &emsp; Yes, in some way. Whenever they are in the same column or row as some piece of gold, they try to go for it, if not they try to explore. But sometimes they don't try not to fall into traps.\n",
    "    <li><b>What is better to pick all the gold in the environment? Less or more steps?:</b></li>\n",
    "        &emsp; It depends of the number of pieces of gold in the environment, if there is a small number of gold then is better to set a small number of steps for an agent to perform because if we take all the gold ang the agent doesn't stop, it would continue to loss performance. But fortunately, the agents in this exercise do stop when there are no more gold left.\n",
    "    <li><b>Was it fair to test with gold pieces and traps in fixed positions? Why not in random positions?:</b></li>\n",
    "        &emsp; That would not have been fair because one agent may have had a more difficult layout than other.\n",
    "</ul>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}