{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize all the libraries which are necessary.\n",
    "\n",
    "Some of the classes imported are the following:\n",
    "<ul>\n",
    "    <li><b>FullyObservableEnvironment:</b></li>\n",
    "        &emsp; This class contains the type of environment where you can perceive all places where there are Golds and Traps. All other relevant portions of the environment are also visible.\n",
    "    <li><b>PartiallyObservableEnvironment:</b></li>\n",
    "        &emsp; In this type of environment some states are hidden, that means, the agent(s) can never see the entire state of the environment. This kind of environment needs agents with memory to be solved.\n",
    "    <li><b>ReflexAgent:</b></li>\n",
    "        &emsp; This class implements the Simple Reflex Agents which acts only on basis of the percepts that the agents receives from the environment. It's actions are based on condition-action rules.\n",
    "    <li><b>ModelBasedAgent:</b></li>\n",
    "        &emsp; This is the kind of agents which maintains the structure that describes the part of the world which cannot see. This knowledge is what is called model of the world.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from FullyObservableEnvironment import FullyObservableEnvironment\n",
    "from PartiallyObservableEnvironment import PartiallyObservableEnvironment\n",
    "from ReflexAgent import ReflexAgent\n",
    "from ModelBasedAgent import ModelBasedAgent\n",
    "from Objects import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Partially Observable Environment</b></h1>\n",
    "\n",
    "The first Agent to be tested is the Reflex Agent in a Partially Observable Environment.\n",
    "In addition to the agent, we also add 5 pieces of gold and 6 traps in specified positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (U - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- 1 1) (- - -) \n(- - -) (U - -) (- - -) \n(- - -) (- - -) (- - 1) \n\nAgent state: (3, 3, UP)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, UP)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- 2 -) \n(- - -) (U - -) (- - -) \n(- - -) (- - -) (- - -) \n\n<STEP 2>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (U - -) (- 2 -) \n(- - -) (- - -) (- - -) \n\n<STEP 3>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (R - -) (- 2 -) \n(- - -) (- - -) (- - -) \n\n<STEP 4>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (R 1 -) \n(- - -) (- - -) \n\n<STEP 5>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 121\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (R - -) \n(- - -) (- - -) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 120\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (D - -) \n(- - -) (- - -) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 119\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (D - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (D - -) \n(- - -) (- - -) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 4, DOWN)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (D - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (D - -) \n(- - -) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, LEFT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (L - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (L - -) \n(- - -) (- - 1) \n\n<STEP 10>\nStep in visited cell\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, LEFT)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (L - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - -) (- - -) \n(- - -) (- - -) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (L - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - 3) (L - -) (- - -) \n(- - -) (- - -) (- - -) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (L - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - 2) (- - -) \n(- 1 -) (- - -) (- - -) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, UP)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (U - 1) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (U - 1) (- - -) \n(- 1 -) (- - -) (- - -) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, RIGHT)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (R - -) (- - -) \n(- 1 -) (- - -) (- - -) \n\n<STEP 15>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, DOWN)\nAgent performance: 94\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (D - -) (- - -) \n(- 1 -) (- - -) (- - -) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- 1 -) (D - -) (- - -) \n\n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- 1 -) (L - -) (- - -) \n\n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(L - -) (- - -) \n\n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (4, 0, UP)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(U - -) (- - -) \n\n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(U - -) (- - -) \n(- - -) (- - -) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - 1) (- - -) \n(U - -) (- - -) \n(- - -) (- - -) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- 1 -) \n(U - -) (- - -) \n(- - -) (- - -) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(U - -) (- 1 -) \n(- - -) (- - -) \n\n<STEP 24>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(R - -) (- 1 -) \n(- - -) (- - -) \n\n<STEP 25>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (R - -) (- - -) \n(- - -) (- - -) (- - -) \n\n"
    }
   ],
   "source": [
    "environment = PartiallyObservableEnvironment()\n",
    "\n",
    "reflex_agent = ReflexAgent()\n",
    "environment.add_thing(reflex_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second agent in the Partially Observable Environment is the Model Based Agent which will be tested with gold and traps at the same positions as the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " - -) (- - -) (- - -) \n(- - -) (U - -) (- 2 -) \n(- - -) (- 1 1) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (- 2 -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\nAgent state: (1, 3, UP)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (R - -) (- 2 -) \n(- - -) (- 1 1) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (- 2 -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 2>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (R 1 -) \n(- 1 1) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V 1 -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 3>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (R - -) \n(- 1 1) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 4>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- - -) (D - -) \n(- 1 1) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 5>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 4, DOWN)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (D - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- 1 1) (D - -) \n(- - -) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (V - -) \n(? ? ?) (? ? ?) (? ? ?) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 6>\nSELECTED ACTION:  TURN\nAgent state:  (2, 4, LEFT)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (L - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(- 1 1) (L - -) \n(- - -) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (- 1 1) (V - -) \n(? ? ?) (? ? ?) (? ? ?) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, LEFT)\nAgent performance: 119\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (V - -) (V - -) \n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 2, LEFT)\nAgent performance: 118\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (L - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - -) (- - -) \n(- - 3) (- - -) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(? ? ?) (- - -) (- - -) (V - -) (V - -) \n(? ? ?) (- - -) (V - -) (V - -) (V - -) \n(? ? ?) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, LEFT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (L - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - 1) (- - -) (- - -) \n(- - -) (L - -) (- - -) \n(- - -) (- - 3) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (2, 1, UP)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - 1) (- - -) (- - -) \n(- - -) (U - -) (- - -) \n(- - -) (- - 3) (- - -) \n\nAgent internal state\n(? ? ?) (? ? ?) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, UP)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (U - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- 1 -) (- - -) \n(- - 1) (U - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, UP)\nAgent performance: 124\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (U - -) (- - -) \n(- - 1) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 13>\nSELECTED ACTION:  TURN\nAgent state:  (0, 1, RIGHT)\nAgent performance: 123\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (R - -) (- - -) \n(- - 1) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 2, RIGHT)\nAgent performance: 122\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (R - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (R - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (- - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 3, RIGHT)\nAgent performance: 121\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (R - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 16>\nSELECTED ACTION:  TURN\nAgent state:  (0, 3, DOWN)\nAgent performance: 120\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n\n(- - -) (D - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, DOWN)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (D - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (D - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 18>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (D - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (- - -) (- - -) \n(? ? ?) (? ? ?) (? ? ?) (? ? ?) (? ? ?) \n\n<STEP 19>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, DOWN)\nAgent performance: 113\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (D - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (D - -) (- - -) \n(- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (V - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  TURN\nAgent state:  (3, 3, LEFT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (L - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - -) (- - -) \n(- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (- - -) (V - -) (- - -) \n(? ? ?) (? ? ?) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 2, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (L - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - 3) (L - -) (- - -) \n(- - -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - 3) (V - -) (V - -) (- - -) \n(? ? ?) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, LEFT)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (L - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (L - 2) (- - -) \n(- 1 -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - 2) (V - -) (V - -) (- - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 23>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (U - 1) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (U - 1) (- - -) \n(- 1 -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - 1) (V - -) (V - -) (- - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 24>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, RIGHT)\nAgent performance: 93\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (R - -) (- - -) \n(- 1 -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 25>\nSELECTED ACTION:  TURN\nAgent state:  (3, 1, DOWN)\nAgent performance: 92\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- - -) (D - -) (- - -) \n(- 1 -) (- - -) (- - -) \n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 26>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 91\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- 1 -) (D - -) (- - -) \n\n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 27>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 90\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) (- - -) \n(- 1 -) (L - -) (- - -) \n\n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 28>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nPercept\n(- - -) (- - -) \n(L - -) (- - -) \n\n\nAgent internal state\n(- - -) (V - -) (V - -) (V - -) (- - -) \n(- - 1) (V - -) (- - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (V - -) (V - -) (- - -) \n(V - -) (V - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = PartiallyObservableEnvironment()\n",
    "\n",
    "model_agent = ModelBasedAgent()\n",
    "environment.add_thing(model_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the implementation of the agents in the Partially Observable Environment we see the results of the <u>Reflex Agent's</u> performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "99"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "reflex_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the <u>Model-Based Agent</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "99"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model_agent.performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Fully Observable Environment</b></h1>\n",
    "\n",
    "In this second part of the homework we use the Fully Observable Environment, first with the Reflex Agent inside it, as well as the past exercise, we use gold and traps in explicit positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (U - -) (- - -) (- - -) (- - 1) \n\nAgent state: (4, 1, UP)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (R - -) (- - -) (- - -) (- - 1) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, DOWN)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\n<STEP 3>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\n<STEP 4>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 5>\nSELECTED ACTION:  TURN\nAgent state:  (4, 0, UP)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (U - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 6>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 0, UP)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (U - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 0, UP)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (U - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 8>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 0, UP)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (U - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 0, UP)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (U - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 10>\nSELECTED ACTION:  TURN\nAgent state:  (0, 0, RIGHT)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (R - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, RIGHT)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 12>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 2, RIGHT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (R - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 3, RIGHT)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (R - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  TURN\nAgent state:  (0, 3, DOWN)\nAgent performance: 101\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, DOWN)\nAgent performance: 100\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (D - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, DOWN)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (D - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, LEFT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (L - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (2, 3, UP)\nAgent performance: 102\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 19>\nStep in visited cell\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 107\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - -) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- - -) (- - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = FullyObservableEnvironment()\n",
    "\n",
    "reflex_agent = ReflexAgent()\n",
    "environment.add_thing(reflex_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Model Based Agent in the Fully Observable Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "---------------------------\nInitial State\n---------------------------\n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (U - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (- - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent state: (3, 4, UP)\nAgent performance: 100\n\n---------------------------\nRun details\n---------------------------\n<STEP 1>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, RIGHT)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (R - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (- - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 2>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, DOWN)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (D - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (- - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 3>\nSELECTED ACTION:  TURN\nAgent state:  (3, 4, LEFT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (L - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (- - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 4>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 3, LEFT)\nAgent performance: 96\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (L - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 5>\nSELECTED ACTION:  TURN\nAgent state:  (3, 3, UP)\nAgent performance: 95\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- 1 1) (- - -) \n\n3 (- - -) (- - 3) (- - -) (U - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (- 1 1) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 6>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 3, UP)\nAgent performance: 99\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (U - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (- - -) (- 2 -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 7>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, UP)\nAgent performance: 98\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (U - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (- 2 -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 8>\nSELECTED ACTION:  TURN\nAgent state:  (1, 3, RIGHT)\nAgent performance: 97\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (R - -) (- 2 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (- 2 -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 9>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 4, RIGHT)\nAgent performance: 106\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R 1 -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V 1 -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 10>\nSELECTED ACTION:  STAY\nAgent state:  (1, 4, RIGHT)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (R - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 11>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (D - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 12>\nSELECTED ACTION:  TURN\nAgent state:  (1, 4, LEFT)\nAgent performance: 114\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (L - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 13>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 3, LEFT)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (L - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (- - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 14>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 2, LEFT)\nAgent performance: 110\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (L - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (- - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 15>\nSELECTED ACTION:  ADVANCE\nAgent state:  (1, 1, LEFT)\nAgent performance: 109\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (L - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 16>\nSELECTED ACTION:  TURN\nAgent state:  (1, 1, UP)\nAgent performance: 108\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- 1 -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (U - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (- 1 -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 17>\nSELECTED ACTION:  ADVANCE\nAgent state:  (0, 1, UP)\nAgent performance: 117\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (U - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 18>\nSELECTED ACTION:  TURN\nAgent state:  (0, 1, RIGHT)\nAgent performance: 116\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (R - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 19>\nSELECTED ACTION:  TURN\nAgent state:  (0, 1, DOWN)\nAgent performance: 115\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 20>\nSELECTED ACTION:  ADVANCE\nAgent state:(1, 1, DOWN)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (D - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (- - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 21>\nSELECTED ACTION:  ADVANCE\nAgent state:  (2, 1, DOWN)\nAgent performance: 111\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (D - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 3) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (- - -) (V - -) (- - -) \n(- - -) (- - 3) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 22>\nSELECTED ACTION:  ADVANCE\nAgent state:  (3, 1, DOWN)\nAgent performance: 105\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (D - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (- - -) (V - -) (- - -) \n(- - -) (V - 2) (- - -) (V - -) (V - -) \n(- 1 -) (- - -) (- - -) (- - -) (- - 1) \n\n<STEP 23>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 1, DOWN)\nAgent performance: 104\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (D - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (- - -) (V - -) (- - -) \n(- - -) (V - 2) (- - -) (V - -) (V - -) \n(- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 24>\nSELECTED ACTION:  TURN\nAgent state:  (4, 1, LEFT)\nAgent performance: 103\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (- 1 -) (L - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (- - -) (V - -) (- - -) \n(- - -) (V - 2) (- - -) (V - -) (V - -) \n(- 1 -) (V - -) (- - -) (- - -) (- - 1) \n\n<STEP 25>\nSELECTED ACTION:  ADVANCE\nAgent state:  (4, 0, LEFT)\nAgent performance: 112\n\nEnvironment: \n     0       1       2       3       4\n  (A G T) (A G T) (A G T) (A G T) (A G T)\n0 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n1 (- - 1) (- - -) (- - -) (- - -) (- - -) \n\n2 (- - -) (- - -) (- - -) (- - -) (- - -) \n\n3 (- - -) (- - 2) (- - -) (- - -) (- - -) \n\n4 (L - -) (- - -) (- - -) (- - -) (- - 1) \n\nAgent internal state\n(- - -) (V - -) (- - -) (- - -) (- - -) \n(- - 1) (V - -) (V - -) (V - -) (V - -) \n(- - -) (V - -) (- - -) (V - -) (- - -) \n(- - -) (V - 2) (- - -) (V - -) (V - -) \n(V - -) (V - -) (- - -) (- - -) (- - 1) \n\n"
    }
   ],
   "source": [
    "environment = FullyObservableEnvironment()\n",
    "\n",
    "model_agent = ModelBasedAgent()\n",
    "environment.add_thing(model_agent)\n",
    "\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (4,0))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (0,1))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (2,3))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "gold = Gold()\n",
    "environment.add_thing(gold, (1,4))\n",
    "\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (1,0))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (3,1))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (2,3))\n",
    "trap = Trap()\n",
    "environment.add_thing(trap, (4,4))\n",
    "\n",
    "environment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the performance of the <u>Reflex Agent</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "117"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "reflex_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the <u>Model Based Agent</u> in the Fully Observable Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "112"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model_agent.performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Conclusions</h1>\n",
    "\n",
    "<ul>\n",
    "    <li><b>Which agent behaves better in the Partially Observable Environment?:</b></li>\n",
    "        &emsp; During the tests that were carried out, we obtained better results in the vast majority of them using the Model-Based Agent which makes sense since it may not receive the full state of the environment and may not be able to see the gold pieces it is looking for, but it keep in his model some of the gold pieces already seen by it's percepts. In the case of the Reflex Agent, when it doesn't perceive any piece of gold it must explore the world which may lead to falling into traps.\n",
    "    <li><b>Which agent behaves better in the Fully Observable Environment?:</b></li>\n",
    "        &emsp; Using this kind of environment, both Agents had similar results because they didn't have to look for pieces of gold, their perceives always had the exact position of each gold in the Environment.\n",
    "    <li><b>Are the Agents behaving rationally?:</b></li>\n",
    "        &emsp; Yes, in some way. Whenever they are in the same column or row as some piece of gold, they try to go for it, if not they try to explore. But sometimes they don't try not to fall into traps.\n",
    "    <li><b>What is better to pick all the gold in the environment? Less or more steps?:</b></li>\n",
    "        &emsp; It depends of the number of pieces of gold in the environment, if there is a small number of gold then is better to set a small number of steps for an agent to perform because if we take all the gold ang the agent doesn't stop, it would continue to loss performance. But fortunately, the agents in this exercise do stop when there are no more gold left.\n",
    "    <li><b>Was it fair to test with gold pieces and traps in fixed positions? Why not in random positions?:</b></li>\n",
    "        &emsp; That would not have been fair because one agent may have had a more difficult layout than other.\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}